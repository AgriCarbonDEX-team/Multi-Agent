import os
from crewai import Agent, Task, Crew
from langchain_openai import ChatOpenAI
from crewai_tools import ScrapeWebsiteTool
from crewai import LLM, Agent, Crew, Process, Task
# from crewai.knowledge.source.crew_docling_source import CrewDoclingSource
from crewai.knowledge.source.pdf_knowledge_source import PDFKnowledgeSource
from langchain_community.chat_models import ChatOllama
from langchain_community.llms import HuggingFaceHub
from huggingface_hub import login
login("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")
os.environ["OPENAI_API_KEY"] = ""



embedder={
"provider": "ollama",
"config": {
    "model": "nomic-embed-text",
    "api_key": "ABCD",
    }
}

# manager_llm = ChatOllama(
#   model="ollama/gemma3:27b",
#   base_url="http://localhost:11434"
# )

llm_local = ChatOllama(
  model="ollama/llama3.1:8b",
  base_url="http://localhost:11434"
)



from crewai import LLM
# OPENAI_API_KEY="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"

manager_llm = LLM(
    model="openai/gpt-4", # call model by provider/model_name
    temperature=0.8,
    max_tokens=250,
    top_p=0.9,
    frequency_penalty=0.1,
    presence_penalty=0.1,
    stop=["END"],
    seed=42
)


# 2. Knowledge from PDF
# =======================
content_source = PDFKnowledgeSource(
    file_paths=["AgriCarbonDEX.pdf"]  # ho·∫∑c ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c tr√™n m√°y b·∫°n
)


RAG_agent = Agent(
    name="RAG Agent",  # <- TH√äM T√äN R√ï R√ÄNG
    role="RAG Analyst",
    goal="Hi·ªÉu r√µ h·ªá th·ªëng AgriCarbonDEX v√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan.",
    backstory="B·∫°n l√† chuy√™n gia n·∫Øm r√µ to√†n b·ªô t√†i li·ªáu AgriCarbonDEX.",
    llm=llm_local,
    embedder = embedder,
    knowledge_sources=[content_source],
    allow_delegation=True,
    verbose=True,
    max_iter=2,
)

rag_task = Task(
    description="Tr·∫£ l·ªùi c√¢u h·ªèi sau v·ªÅ h·ªá th·ªëng AgriCarbonDEX: {question}",
    expected_output="M·ªôt ƒëo·∫°n vƒÉn gi·∫£i th√≠ch r√µ r√†ng v·ªÅ h·ªá th·ªëng.",
    agent=RAG_agent
)



# =======================
# =======================
# 3. Web Scrape Agent
# =======================

# ‚úÖ C√°c c√¥ng c·ª• qu√©t d·ªØ li·ªáu t·ª´ web
tool1 = ScrapeWebsiteTool(website_url="https://www.vinacontrol.com.vn/news/Thi-truong-carbon")
tool2 = ScrapeWebsiteTool(website_url="https://baochinhphu.vn/hinh-thanh-thi-truong-tin-chi-carbon-rung-cua-viet-nam-102240331060535888.htm")

# ‚úÖ ƒê·ªãnh nghƒ©a Agent th·ª±c hi·ªán scraping
scrape_agent = Agent(
    role="Chuy√™n gia thu th·∫≠p d·ªØ li·ªáu ESG",
    goal="T·ª± ƒë·ªông t√¨m ki·∫øm v√† t·ªïng h·ª£p th√¥ng tin ESG, t√≠n ch·ªâ carbon t·ª´ c√°c ngu·ªìn web ƒë√°ng tin c·∫≠y.",
    backstory=(
        "B·∫°n l√† m·ªôt chuy√™n gia gi√†u kinh nghi·ªám trong vi·ªác khai th√°c d·ªØ li·ªáu t·ª´ c√°c trang web "
        "li√™n quan ƒë·∫øn m√¥i tr∆∞·ªùng, t√≠n ch·ªâ carbon v√† ESG. M·ª•c ti√™u c·ªßa b·∫°n l√† tr√≠ch xu·∫•t th√¥ng tin ch√≠nh x√°c "
        "v√† t·∫°o ra b·∫£n t√≥m t·∫Øt s√∫c t√≠ch, d·ªÖ hi·ªÉu cho nh√≥m nghi√™n c·ª©u."
    ),
    tools=[tool1, tool2],
    llm=llm_local,
    allow_delegation=True,
    verbose=True,
    max_iter=2,
)

# ‚úÖ Task t∆∞∆°ng ·ª©ng cho scrape agent
scrape_task = Task(
    description=(
        "T√¨m ki·∫øm v√† t√≥m t·∫Øt th√¥ng tin chi ti·∫øt v·ªÅ ch·ªß ƒë·ªÅ: '{question}'. "
        "D·ª±a tr√™n n·ªôi dung l·∫•y ƒë∆∞·ª£c t·ª´ c√°c trang web, h√£y t·∫°o ra b·∫£n t√≥m t·∫Øt th·ªÉ hi·ªán ƒë·ªãnh nghƒ©a, "
        "√Ω nghƒ©a v√† vai tr√≤ c·ªßa ch·ªß ƒë·ªÅ m·ªôt c√°ch ng·∫Øn g·ªçn, r√µ r√†ng."
    ),
    expected_output=(
        "M·ªôt ƒëo·∫°n t√≥m t·∫Øt kho·∫£ng 5‚Äì10 d√≤ng, tr√¨nh b√†y kh√°i ni·ªám, vai tr√≤ v√† m·ª©c ƒë·ªô li√™n quan c·ªßa ch·ªß ƒë·ªÅ ƒë·∫øn ESG ho·∫∑c t√≠n ch·ªâ carbon."
    ),
    agent=scrape_agent
)

# =======================
# 4. Manager Agent
# =======================
manager = Agent(
    role="Project Manager",
    goal="ƒêi·ªÅu ph·ªëi c√°c chuy√™n gia ƒë·ªÉ ho√†n th√†nh b√°o c√°o ph√¢n t√≠ch h·ªá th·ªëng v√† th·ªã tr∆∞·ªùng.",
    backstory="M·ªôt qu·∫£n l√Ω d√†y d·∫°n, c√≥ kh·∫£ nƒÉng ph√¢n ph·ªëi nhi·ªám v·ª• v√† t·ªïng h·ª£p k·∫øt qu·∫£ hi·ªáu qu·∫£.",
    allow_delegation=True,
    llm=manager_llm, 
    verbose=True,
    max_iter=2,
)

manager_task = Task(
    description=(
        "Ch·ªâ ƒë·∫°o nh√≥m g·ªìm m·ªôt RAG agent v√† m·ªôt Web Scraper ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi sau: '{question}'. "
        "Ph·ªëi h·ª£p c√°c chuy√™n gia ƒë·ªÉ t·ªïng h·ª£p th√†nh m·ªôt b√°o c√°o ƒë·∫ßy ƒë·ªß d·ª±a tr√™n d·ªØ li·ªáu t·ª´ t√†i li·ªáu PDF v√† c√°c website ESG."
    ),
    expected_output="M·ªôt b√°o c√°o ph√¢n t√≠ch t·∫ßm 4-6 d√≤ng li√™n quan ƒë·∫øn '{question}'.",
    agent=manager
)

# =======================
# 5. Crew c·∫•u tr√∫c ph√¢n c·∫•p
# =======================



crew = Crew(
    agents=[manager, RAG_agent, scrape_agent],
    tasks=[manager_task],
    process=Process.hierarchical,
    manager_llm=manager_llm,
    verbose=True,
    max_iter=2  # üõë stop after 5 interactions

)

try:
    result = crew.kickoff(
        inputs={"question": "Th·ªã tr∆∞·ªùng t√≠n ch·ªâ carbon ƒëang nh∆∞ n√†o?"}
    )
    print("======== K·∫æT QU·∫¢ =========")
    print(result)
except Exception as e:
    print("‚ùå ƒê√£ x·∫£y ra l·ªói:")
    print(e)


